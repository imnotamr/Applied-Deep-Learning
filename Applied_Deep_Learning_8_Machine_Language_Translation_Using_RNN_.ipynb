{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyMOuxwPuUM5cT+f0EZyDhiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imnotamr/Applied-Deep-Learning/blob/main/Applied_Deep_Learning_8_Machine_Language_Translation_Using_RNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwoVC68fJzP-"
      },
      "outputs": [],
      "source": [
        "import string                          # To process strings ---> uppercase, lowercase and punctuation\n",
        "import re                              # For Regular Expressions\n",
        "from pickle import dump                # To serialized, store and save data\n",
        "from unicodedata import normalize      # For Unicode text processing ( for strange symbols )\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load document to memory\n",
        "def load_document(filename):\n",
        "  file = open(filename, mode='rt', encoding='utf-8') # rt ---> Read Text and utf-8 ---> encoding for text languages\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "# Split loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "  lines = doc.strip().split('\\n')\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "  return pairs\n",
        "\n",
        "\n",
        "\n",
        "# Clean list of lines\n",
        "def clean_pairs(lines):\n",
        "  # A list to save pairs after cleaning\n",
        "\tcleaned = list()\n",
        "\n",
        "  # Remove nonprintable characters with re library\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\n",
        "  # Use the table to remove any punctuation from texts with string library\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\n",
        "\t\tfor line in pair:\n",
        "      # Processing texts that contain strange characters/symbols\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8') #line = line.decode('unicode-escape')\n",
        "\n",
        "      # Split text into a list of words based on spaces\n",
        "\t\t\tline = line.split()\n",
        "      # Convert every word to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "      # Remove punctuation marks from each word (in table i've created up ^)\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "      # Remove any non printable character from the word\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "      # Make sure that the word consists of letters only, to remove words that contain numbers\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "      # Convert the word list back to text (one sentence)\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn cleaned\n",
        "\n",
        "\n",
        "\n",
        " # Save the list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb')) # W--> write and B--> binary (since the pickle library work on files as binary data)\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "\n",
        " # Load dataset\n",
        "filename = '/content/fra.txt'\n",
        "doc = load_document(filename)\n",
        "\n",
        "pairs = to_pairs(doc)\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "save_clean_data(clean_pairs, 'english-french.pkl')\n",
        "\n",
        "# Check\n",
        "for i in range(100):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i][0], clean_pairs[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0aZwA-QTWsi",
        "outputId": "6cd9a1dd-052c-4941-bb2b-c828f620ef44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-french.pkl\n",
            "[go] => [va]\n",
            "[hi] => [salut]\n",
            "[hi] => [salut]\n",
            "[run] => [cours]\n",
            "[run] => [courez]\n",
            "[who] => [qui]\n",
            "[wow] => [ca alors]\n",
            "[fire] => [au feu]\n",
            "[help] => [a laide]\n",
            "[jump] => [saute]\n",
            "[stop] => [ca suffit]\n",
            "[stop] => [stop]\n",
            "[stop] => [arretetoi]\n",
            "[wait] => [attends]\n",
            "[wait] => [attendez]\n",
            "[go on] => [poursuis]\n",
            "[go on] => [continuez]\n",
            "[go on] => [poursuivez]\n",
            "[hello] => [bonjour]\n",
            "[hello] => [salut]\n",
            "[i see] => [je comprends]\n",
            "[i try] => [jessaye]\n",
            "[i won] => [jai gagne]\n",
            "[i won] => [je lai emporte]\n",
            "[i won] => [jai gagne]\n",
            "[oh no] => [oh non]\n",
            "[attack] => [attaque]\n",
            "[attack] => [attaquez]\n",
            "[cheers] => [sante]\n",
            "[cheers] => [a votre sante]\n",
            "[cheers] => [merci]\n",
            "[cheers] => [tchintchin]\n",
            "[get up] => [levetoi]\n",
            "[go now] => [va maintenant]\n",
            "[go now] => [allezy maintenant]\n",
            "[go now] => [vasy maintenant]\n",
            "[got it] => [jai pige]\n",
            "[got it] => [compris]\n",
            "[got it] => [pige]\n",
            "[got it] => [compris]\n",
            "[got it] => [tas capte]\n",
            "[hop in] => [monte]\n",
            "[hop in] => [montez]\n",
            "[hug me] => [serremoi dans tes bras]\n",
            "[hug me] => [serrezmoi dans vos bras]\n",
            "[i fell] => [je suis tombee]\n",
            "[i fell] => [je suis tombe]\n",
            "[i know] => [je sais]\n",
            "[i left] => [je suis parti]\n",
            "[i left] => [je suis partie]\n",
            "[i lost] => [jai perdu]\n",
            "[i paid] => [jai paye]\n",
            "[im] => [jai ans]\n",
            "[im ok] => [je vais bien]\n",
            "[im ok] => [ca va]\n",
            "[listen] => [ecoutez]\n",
            "[no way] => [cest pas possible]\n",
            "[no way] => [impossible]\n",
            "[no way] => [en aucun cas]\n",
            "[no way] => [sans facons]\n",
            "[no way] => [cest hors de question]\n",
            "[no way] => [il nen est pas question]\n",
            "[no way] => [cest exclu]\n",
            "[no way] => [en aucune maniere]\n",
            "[no way] => [hors de question]\n",
            "[really] => [vraiment]\n",
            "[really] => [vrai]\n",
            "[really] => [ah bon]\n",
            "[thanks] => [merci]\n",
            "[we try] => [on essaye]\n",
            "[we won] => [nous avons gagne]\n",
            "[we won] => [nous gagnames]\n",
            "[we won] => [nous lavons emporte]\n",
            "[we won] => [nous lemportames]\n",
            "[ask tom] => [demande a tom]\n",
            "[awesome] => [fantastique]\n",
            "[be calm] => [sois calme]\n",
            "[be calm] => [soyez calme]\n",
            "[be calm] => [soyez calmes]\n",
            "[be cool] => [sois detendu]\n",
            "[be fair] => [sois juste]\n",
            "[be fair] => [soyez juste]\n",
            "[be fair] => [soyez justes]\n",
            "[be fair] => [sois equitable]\n",
            "[be fair] => [soyez equitable]\n",
            "[be fair] => [soyez equitables]\n",
            "[be kind] => [sois gentil]\n",
            "[be nice] => [sois gentil]\n",
            "[be nice] => [sois gentille]\n",
            "[be nice] => [soyez gentil]\n",
            "[be nice] => [soyez gentille]\n",
            "[be nice] => [soyez gentils]\n",
            "[be nice] => [soyez gentilles]\n",
            "[beat it] => [degage]\n",
            "[call me] => [appellemoi]\n",
            "[call me] => [appellezmoi]\n",
            "[call us] => [appellenous]\n",
            "[call us] => [appeleznous]\n",
            "[come in] => [entrez]\n",
            "[come in] => [entre]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print(f'Saved: {filename}')\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('/content/english-french.pkl')\n",
        "\n",
        "# Ensure the dataset isn't empty before proceeding\n",
        "if len(raw_dataset) == 0:\n",
        "    raise ValueError(\"Dataset is empty. Please check the input file.\")\n",
        "\n",
        "# Reduce dataset size (this line is optional; only reduce if necessary)\n",
        "n_sentences = len(raw_dataset)\n",
        "dataset = raw_dataset[:n_sentences]  # Ensure this slice is meaningful\n",
        "\n",
        "# Shuffle the dataset\n",
        "shuffle(dataset)\n",
        "\n",
        "# Check if dataset is empty after shuffling\n",
        "if len(dataset) == 0:\n",
        "    raise ValueError(\"Dataset is empty after shuffling. Please check the input file.\")\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "split_index = int(0.8 * len(dataset))  # 80% for training, 20% for testing\n",
        "\n",
        "# Ensure both train and test datasets are not empty\n",
        "train, test = dataset[:split_index], dataset[split_index:]\n",
        "if len(train) == 0 or len(test) == 0:\n",
        "    raise ValueError(\"Training or testing set is empty. Adjust the split ratio or ensure enough data.\")\n",
        "\n",
        "# Save the cleaned and split datasets\n",
        "save_clean_data(dataset, 'english-french-both.pkl')\n",
        "save_clean_data(train, 'english-french-train.pkl')\n",
        "save_clean_data(test, 'english-french-test.pkl')\n",
        "\n",
        "# Check the dataset sizes\n",
        "print(f\"Train set size: {len(train)}\")\n",
        "print(f\"Test set size: {len(test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR16RAeFV9Zo",
        "outputId": "6453bd5e-39bb-4705-bd1b-92ccce3fe0af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-french-both.pkl\n",
            "Saved: english-french-train.pkl\n",
            "Saved: english-french-test.pkl\n",
            "Train set size: 14009\n",
            "Test set size: 3503\n"
          ]
        }
      ]
    }
  ]
}